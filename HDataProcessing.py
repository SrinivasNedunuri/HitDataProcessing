from collections import defaultdict
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import csv
import argparse
import csv
import configparser
import time

config = configparser.ConfigParser()
config.read('config.conf')
EXPECTED_COLUMNS = config['mapping']['known_columns'].split('\n')
"""list: columns that are excepted in the hitdata file. Read from config file"""
MANDATORY_COLUMNS = set(['event_list', 'product_list', 'referrer', 'hit_gmt_time'])
"""set: set of mandatory columns required for generating the desired output"""
OUTPUT_HEADER = ['Search Engine Domain', 'Search Keyword', 'Revenue']
"""list of str: Desired output header"""
PROUDUCT_ATTR = ['Category', 'Product Name', 'Number of Items', 'Total Revenue', 'Custom Event', 'Merchandising eVar']
"""list of str: List of product attributes"""
EVENTS = {
          '1': 'Purchase',
          '2': 'Product view',
          '10': 'Shopping Cart Open',
          '11': 'Shopping Cart Checkout',
          '12': 'Shopping Cart Add',
          '13': 'Shopping Cart Remove',
          '14': 'Shopping Cart View'
         }
"""dict: event description by event_id"""
OUTPUT_FILENAME = datetime.today().strftime('%Y-%m-%d') + '_SearchKeywordPerformer.tab'

class HitDataProcessor:
    """A class to represent Hitdata file object and process it.

    """

    def __init__(self, dict_reader_obj, output_header, product_attr, event_list, output_f, client='esshopzilla.com'):
        """Args:
            dict_reader_obj (:obj:csv.DictReader): iterator yields each row as ordered dictionary.
            output_header (list) : Desired output header
            product_attr (list): List of product attributes
            event_list (dict): event description by event_id
            client (str): client website.
            output_f (str): output file name

        """
        self.reader = dict_reader_obj
        self.output_header = output_header
        self.product_attr = product_attr
        self.event_list = event_list
        self.client = client
        #: list of str: list of columns read from reader object
        self.columns = self.reader.fieldnames
        #: str: output filename based on execution date
        self.output_filename = output_f
        #: dict of list: to hold list of processed referrer and purchase details by ip address
        self.processed_rows = defaultdict(list)
        #: dist of dict: to store revenue generated by keyword by search engine domain.
        self.rev_by_ref_keyword = defaultdict(lambda: defaultdict(int))

    def write_output(self, list_of_rows):
        """Method to write out list of rows to an output path specified

        Args:
            list_of_rows (list) : list of tuples

        """
        with open(self.output_filename, 'w') as out_file:
            writer_obj = csv.writer(out_file, delimiter='\t')
            writer_obj.writerow(self.output_header)
            writer_obj.writerows(line for line in list_of_rows)

    def process_rows(self):
        """processes raw data, builds processed rows and revenue by keyword instance dictionaries.

        Iterates the reader object, builds instance attribute "processed_rows" dictionary by calling methods
        "get_processed_purchase" and "get_processed_referrer".

        Two types of processed rows possible. Ex:
        1. Referrer_row: {'referrer': 'google.com', 'keyword': 'ipod', 'time': 1254033280}
        2. Purchase_row: {'Category': 'Electronics', 'Product Name': 'Zune - 32GB', 'Number of Items': '1',
                        'Total Revenue': '250', 'Custom Event': '', 'Merchandising eVar': '', 'time': 1254034666}

        These two processed rows are appended to the processed_rows[ip]
        Note: "ip address" links these two rows.

        Finally, "calls the process_revenue_by_keyword" method to build "rev_by_ref_keyword"
        rev_by_ref_keyword look like: {
                                        'google.com': defaultdict(<class 'int'>, {'ipod': 480}),
                                        'search.yahoo.com': defaultdict(<class 'int'>, {'cd player': 0})
                                        }

        """
        for row in self.reader:
            if self.event_list.get(row['event_list']) == 'Purchase':
                purchase_processed = self.get_processed_purchase(row)
                self.processed_rows[row['ip']].append(purchase_processed)
                self.process_revenue_by_keyword(row['ip'], purchase_processed, row_type='purchase')
            else:
                referrer_processed = self.get_processed_referrer(row)
                if referrer_processed:
                    self.processed_rows[row['ip']].append(referrer_processed)
                    self.process_revenue_by_keyword(row['ip'], referrer_processed, row_type='referrer')

    def get_processed_purchase(self, row):
        """Parses the product_list column in row and returns dictionary of product attributes with values

        Args:
            row (dict): with column name as key and associated value

        Returns:
            dict: {
                   'Category': 'Electronics',
                   'Product Name': 'Zune - 32GB',
                   'Number of Items': '1',
                   'Total Revenue': '250',
                   'Custom Event': '',
                   'Merchandising eVar': '',
                   'time': 1254034666
                   }

        """
        product_dict = {}
        product_list = row['product_list'].split(';')
        for i in range(len(self.product_attr)):
            try:
                product_dict[self.product_attr[i]] = product_list[i]
            except IndexError:
                product_dict[self.product_attr[i]] = ''
        product_dict.update({'time': int(row['hit_time_gmt'])})
        return product_dict

    def get_processed_referrer(self, row):
        """Parses the referrer column in the row and returns dictionary of referrer attributes with values

        Args:
            row (dict): with column name as key and associated value

        Returns:
            dict: {'referrer': domain, 'keyword': keyword, 'time': int(row['hit_time_gmt'])}

        """
        parse = urlparse(row['referrer'])
        domain = self.get_url_domain(parse)
        if domain != self.client:
            keyword = self.get_url_keyword(parse)
            return {'referrer': domain, 'keyword': keyword, 'time': int(row['hit_time_gmt'])}

    def process_revenue_by_keyword(self, ip,  row_processed, row_type):
        """Adds the revenue by keyword using processed row

        Note:
            1. Assuming no overlapping sessions for an ip (i.e) if a session starts, new session cannot be started until
               first session(that involves purchase event) ends for an ip.
            2. Assuming data is ordered by hit_gmt_time in ascending order.

        Args:
            ip (str) : ip address of the session
            row_processed (dict) : processed referrer row/ processed purchase row
            row_type (str): referrer/purchase

        """
        if row_type == 'referrer':
            domain = row_processed['referrer']
            keyword = row_processed['keyword']
            self.rev_by_ref_keyword[domain][keyword] += 0
        else:
            try:
                referrer_key = self.processed_rows[ip][-2]  #: get referrer key from last but one index
                domain = referrer_key['referrer']
                keyword = referrer_key['keyword']
                self.rev_by_ref_keyword[domain][keyword] += int(row_processed['Total Revenue'])
            except (IndexError, KeyError):
                print('purchase is not associated with referrer')

    def sort_by_revenue(self):
        """ Sorts the revenue by keyword dictionary by revenue

        Returns:
            list: list of search keyword - highest revenue at top and lowest at bottom.

        """
        output = []
        lis = self.rev_by_ref_keyword.items()
        for key, value in lis:
            for k, v in value.items():
                output.append(tuple([key, k, v]))
        output.sort(key=lambda x: x[2], reverse=True)
        return output

    @staticmethod
    def get_url_domain(parser):
        """Static Method to get domain from parser object

        Args:
            parser (object) : parser object from urlparse

        Returns:
            'str' : search engine domain

        """
        if parser.netloc.startswith('www.'):
            return parser.netloc[4:]
        else:
            return parser.netloc

    @staticmethod
    def get_url_keyword(parser):
        """Static Method to get keyword parser object

           Args:
               parser (object) : parser object from urlparse

           Returns:
               'str' : Keyword

       """
        tags = parse_qs(parser.query)
        keyword = []
        if tags.get('q'):
            keyword = tags['q']
        if tags.get('p'):
            keyword = tags['p']
        return keyword[0].lower()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-f', '--filename', help='Hit data file to be processed', required=True)
    args = parser.parse_args()
    file = args.filename
    # if file not found or file format is not right or path specified is wrong raise IO error
    try:
        with open(file) as tsv_file:
            reader = csv.DictReader(tsv_file, delimiter='\t')
            # instantiate the Hitdate object
            hitdata = HitDataProcessor(reader, OUTPUT_HEADER, PROUDUCT_ATTR, EVENTS, OUTPUT_FILENAME)
            # check for extra columns in the dataset
            print('Extra columns in file', set(hitdata.columns) - set(EXPECTED_COLUMNS))
            # check for missing columns in the dataset
            missing_cols = set(EXPECTED_COLUMNS) - set(hitdata.columns)
            print('Missing columns in file', missing_cols)
            # raise an error if any mandatory fields are missing
            for col in MANDATORY_COLUMNS:
                if col in missing_cols:
                    raise ValueError('Mandatory field for processing HitData is missing')
            start = time.time()
            # process data
            hitdata.process_rows()
            # take output list
            output_rows = hitdata.sort_by_revenue()
            # write output list to a file
            hitdata.write_output(output_rows)
            end = time.time()
            print("Took %f ms" % ((end - start) * 1000.0))

    except IOError as e:
        print('Provided filename/file location is not valid -->', e)


if __name__ == '__main__':
    main()